{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+4Z0swYge6HWWV031xptf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaioHenrique28/TCC_AMAN_osint/blob/main/1_coleta_massiva.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TCC: Análise de Sentimentos - Exército Brasileiro (NLP)**\n",
        "**Autor:** Cadete Caio Henrique\n",
        "**Módulo 1:** Coleta de Dados Automatizada (ETL)\n",
        "\n",
        "**Objetivo deste Notebook:**\n",
        "Executar a etapa de extração de dados brutos (*Raw Data*) prevista na metodologia. O script conecta-se à **YouTube Data API v3**, itera sobre uma lista de vídeos estratégicos selecionados e extrai todos os comentários disponíveis, contornando a limitação de paginação da API.\n",
        "\n",
        "**Metodologia Aplicada:**\n",
        "* **Fonte de Dados:** Comentários públicos em canais oficiais (OSINT).\n",
        "* **Ferramenta:** Python (`google-api-python-client`).\n",
        "* **Armazenamento:** Google Drive (Formato `.csv`)."
      ],
      "metadata": {
        "id": "UtZrm2I2KwvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwYh4Gb6DP_1",
        "outputId": "2ff1d52a-3ad3-4a75-de0a-1826edff300d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--- INICIANDO OPERAÇÃO DE COLETA ---\n",
            "--> Iniciando extração do vídeo: sqS2JDhTOiQ\n",
            "    [OK] 301 comentários extraídos.\n",
            "--> Iniciando extração do vídeo: BY2hsjPervo\n",
            "    [OK] 164 comentários extraídos.\n",
            "--> Iniciando extração do vídeo: s9vjDzPd29s\n",
            "    [OK] 563 comentários extraídos.\n",
            "\n",
            "========================================\n",
            "MISSÃO CUMPRIDA!\n",
            "Total acumulado: 1028 comentários.\n",
            "Arquivo salvo em: /content/drive/MyDrive/TCC_AMAN_2025/base_dados_bruta.csv\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# --- SCRIPT DE COLETA MASSIVA (ETL V2.1 - CORRIGIDO) ---\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from googleapiclient.discovery import build\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. MONTAGEM DO DRIVE\n",
        "# Se já estiver montado, ele apenas avisa.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. CONFIGURAÇÕES\n",
        "API_KEY = \"INSIRA_SUA_CHAVE_AQUI\"  # <--- RECOLOQUE SUA CHAVE AQUI\n",
        "ALVOS_VIDEOS = [\n",
        "    \"sqS2JDhTOiQ\",  # Enchentes RS\n",
        "    \"BY2hsjPervo\",  # Vídeo 2 (exemplo)\n",
        "    \"s9vjDzPd29s\"   # Vídeo 3 (exemplo)\n",
        "]\n",
        "\n",
        "def coletar_comentarios_video(youtube_service, video_id):\n",
        "    \"\"\"\n",
        "    Função que baixa TODOS os comentários de um único vídeo.\n",
        "    \"\"\"\n",
        "    comentarios = [] # <--- A lista nasce com nome em PORTUGUÊS\n",
        "    print(f\"--> Iniciando extração do vídeo: {video_id}\")\n",
        "\n",
        "    try:\n",
        "        request = youtube_service.commentThreads().list(\n",
        "            part=\"snippet\",\n",
        "            videoId=video_id,\n",
        "            maxResults=100,\n",
        "            textFormat=\"plainText\"\n",
        "        )\n",
        "\n",
        "        while request:\n",
        "            response = request.execute()\n",
        "\n",
        "            for item in response['items']:\n",
        "                dado = item['snippet']['topLevelComment']['snippet']\n",
        "                comentarios.append({\n",
        "                    'Video_ID': video_id,\n",
        "                    'Autor': dado.get('authorDisplayName', 'Desconhecido'),\n",
        "                    'Data': dado.get('publishedAt'),\n",
        "                    'Comentario': dado.get('textDisplay'),\n",
        "                    'Likes': dado.get('likeCount', 0)\n",
        "                })\n",
        "\n",
        "            if 'nextPageToken' in response:\n",
        "                request = youtube_service.commentThreads().list(\n",
        "                    part=\"snippet\",\n",
        "                    videoId=video_id,\n",
        "                    pageToken=response['nextPageToken'],\n",
        "                    maxResults=100,\n",
        "                    textFormat=\"plainText\"\n",
        "                )\n",
        "                time.sleep(0.5)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        print(f\"    [OK] {len(comentarios)} comentários extraídos.\")\n",
        "        return comentarios # <--- CORREÇÃO: Agora retorna a variável certa (antes estava 'comments')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    [ERRO] Falha no vídeo {video_id}: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- EXECUÇÃO PRINCIPAL ---\n",
        "def main():\n",
        "    print(\"--- INICIANDO OPERAÇÃO DE COLETA ---\")\n",
        "\n",
        "    youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
        "    todos_dados = []\n",
        "\n",
        "    for video in ALVOS_VIDEOS:\n",
        "        dados_video = coletar_comentarios_video(youtube, video)\n",
        "        todos_dados.extend(dados_video)\n",
        "\n",
        "    df_final = pd.DataFrame(todos_dados)\n",
        "\n",
        "    if not df_final.empty:\n",
        "        pasta_destino = '/content/drive/MyDrive/TCC_AMAN_2025'\n",
        "        if not os.path.exists(pasta_destino):\n",
        "            os.makedirs(pasta_destino)\n",
        "\n",
        "        caminho_arquivo = f'{pasta_destino}/base_dados_bruta.csv'\n",
        "        df_final.to_csv(caminho_arquivo, index=False)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(f\"MISSÃO CUMPRIDA!\")\n",
        "        print(f\"Total acumulado: {len(df_final)} comentários.\")\n",
        "        print(f\"Arquivo salvo em: {caminho_arquivo}\")\n",
        "        print(\"=\"*40)\n",
        "    else:\n",
        "        print(\"\\n[FALHA] Nenhum dado coletado.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Detalhamento da Arquitetura do Script**\n",
        "O código desenvolvido acima implementa um pipeline de **ETL (Extract, Transform, Load)** robusto para a coleta de dados não estruturados (texto), seguindo as melhores práticas de Engenharia de Dados. Abaixo, detalha-se o funcionamento de cada módulo lógico:\n",
        "\n",
        "### **A. Módulo de Extração (Extract)**\n",
        "* **Conexão Segura:** Utilizamos a biblioteca `google-api-python-client` para estabelecer um túnel autenticado com a API do YouTube v3. A segurança é garantida via `API_KEY`.\n",
        "* **Paginação Recursiva:** A API do YouTube possui uma limitação técnica que entrega apenas 100 comentários por requisição. Para contornar isso, implementamos um loop `while` que verifica a existência do token `nextPageToken`. Enquanto houver um token, o script continua solicitando a próxima página, garantindo a extração **exaustiva** (todos os comentários, não apenas os recentes).\n",
        "* **Blindagem de Erros:** O bloco `try-except` protege a execução. Caso um vídeo específico tenha sido deletado ou tenha comentários desativados, o script registra o erro, mas não interrompe a coleta dos demais vídeos da lista.\n",
        "\n",
        "### **B. Módulo de Transformação (Transform)**\n",
        "* **Parsing de JSON:** A resposta bruta da API é um objeto JSON aninhado e complexo. O script navega cirurgicamente por essa estrutura (`item['snippet']['topLevelComment']...`) para extrair apenas os dados de interesse.\n",
        "* **Normalização com `.get()`:** Utilizamos o método `.get()` nos dicionários para prevenir falhas (erros de chave). Se um campo (como 'likeCount') estiver ausente na fonte, o sistema preenche automaticamente com um valor padrão (0), garantindo a integridade tabular.\n",
        "* **Estruturação Tabular:** Os dados, inicialmente em listas de dicionários, são convertidos para um **DataFrame do Pandas**. Essa estrutura facilita a manipulação matemática e estatística nas próximas etapas.\n",
        "\n",
        "### **C. Módulo de Carga (Load)**\n",
        "* **Persistência na Nuvem:** O resultado final não é mantido apenas na memória volátil (RAM). Ele é exportado fisicamente para o **Google Drive** em formato `.csv` (Comma Separated Values).\n",
        "* **Versionamento de Dados:** O arquivo é salvo no diretório `TCC_AMAN_2025/base_dados_bruta.csv`, servindo como ponto de restauração (*checkpoint*). Isso permite que a etapa de Inteligência Artificial (Semana 2) seja iniciada sem a necessidade de reconectar à API do YouTube, economizando cota de requisição.\n",
        "\n",
        "## **Próximos Passos**\n",
        "Com a base de dados bruta salva, o próximo estágio do projeto consistirá em:\n",
        "1.  **Limpeza de Texto (NLP):** Remoção de ruídos (emojis, links, quebras de linha).\n",
        "2.  **Engenharia de Prompt:** Conexão com uma LLM (Gemini/GPT) para classificar o sentimento de cada comentário (Positivo/Negativo/Neutro).\n",
        "3.  **Análise Exploratória:** Geração de estatísticas descritivas sobre a base rotulada."
      ],
      "metadata": {
        "id": "1IZN4hEnLHVD"
      }
    }
  ]
}